{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Étape 3 — Modélisation (Deep Learning)\n",
    "\n",
    "Ce notebook entraîne des modèles de **classification de texte** sur vos tables MySQL (`news` et `labeled`).\n",
    "Nous comparons :\n",
    "- **Baseline TF‑IDF + Logistic Regression** (référence rapide)\n",
    "- **DistilBERT** (fine‑tuning efficace sur CPU/venv)\n",
    "\n",
    "Pourquoi DistilBERT ?\n",
    "- Très bon compromis **performance/ressources** vs BERT\n",
    "- Fine‑tuning rapide en 1–3 epochs, même sans GPU\n",
    "\n",
    "Si vous ajoutez de l'arabe/français sans traduction, utilisez **XLM‑Roberta** (multilingue).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x18ff220b830>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONNEXION & CHARGEMENT\n",
      "======================================================================\n",
      "✓ Connexion MySQL réussie!\n",
      "NEWS text column: text_processed\n",
      "LABELED text column: text_processed\n"
     ]
    }
   ],
   "source": [
    "# Connexion MySQL (fallback CSV)\n",
    "DB_USERNAME = 'root'\n",
    "DB_PASSWORD = ''\n",
    "DB_HOST = 'localhost'\n",
    "DB_PORT = '3306'\n",
    "DB_NAME = 'dataControl'\n",
    "\n",
    "conn_str = f'mysql+pymysql://{DB_USERNAME}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}'\n",
    "print('='*70)\n",
    "print('CONNEXION & CHARGEMENT')\n",
    "print('='*70)\n",
    "try:\n",
    "    engine = create_engine(conn_str)\n",
    "    with engine.connect() as _:\n",
    "        print('✓ Connexion MySQL réussie!')\n",
    "    df_news = pd.read_sql('SELECT * FROM news', engine)\n",
    "    df_labeled = pd.read_sql('SELECT * FROM labeled', engine)\n",
    "except Exception as e:\n",
    "    print(f'✗ Erreur MySQL: {e}')\n",
    "    print('⚠️ Fallback CSV: data/*.csv')\n",
    "    df_news = pd.read_csv('data/fake_news_dataset.csv')\n",
    "    df_labeled = pd.read_csv('data/labeled_data.csv')\n",
    "\n",
    "# Choix des colonnes texte\n",
    "def pick_text_column(df):\n",
    "    for c in ['text_processed','text_cleaned','text','tweet','title']:\n",
    "        if c in df.columns: return c\n",
    "    raise ValueError('Aucune colonne texte trouvée')\n",
    "\n",
    "text_col_news = pick_text_column(df_news)\n",
    "text_col_lab = pick_text_column(df_labeled)\n",
    "print(f'NEWS text column: {text_col_news}')\n",
    "print(f'LABELED text column: {text_col_lab}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEWS labels: fake=1, normal=0\n",
      "LABELED labels: hate=1, normal=0\n"
     ]
    }
   ],
   "source": [
    "# Préparation des labels\n",
    "def map_news_label(v):\n",
    "    v = str(v).lower() if pd.notna(v) else ''\n",
    "    return 1 if v == 'fake' else 0\n",
    "\n",
    "def map_lab_class(v):\n",
    "    # 0: hate_speech, 1: offensive_language -> 1 (hate); 2 neither -> 0 (normal)\n",
    "    try:\n",
    "        vi = int(v)\n",
    "    except Exception:\n",
    "        return 0\n",
    "    return 1 if vi in (0,1) else 0\n",
    "\n",
    "y_news = df_news['label'].apply(map_news_label) if 'label' in df_news.columns else pd.Series([0]*len(df_news))\n",
    "X_news = df_news[text_col_news].astype(str)\n",
    "\n",
    "y_lab = df_labeled['class'].apply(map_lab_class) if 'class' in df_labeled.columns else pd.Series([0]*len(df_labeled))\n",
    "X_lab = df_labeled[text_col_lab].astype(str)\n",
    "\n",
    "print('NEWS labels: fake=1, normal=0')\n",
    "print('LABELED labels: hate=1, normal=0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline — TF‑IDF + Logistic Regression\n",
    "Simple, rapide, utile pour comparer avec des modèles profonds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BASELINE TF-IDF + LR\n",
      "======================================================================\n",
      "[NEWS] Accuracy: 0.5022 | F1: 0.5071\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4995    0.4952    0.4973      1989\n",
      "           1     0.5049    0.5092    0.5071      2011\n",
      "\n",
      "    accuracy                         0.5022      4000\n",
      "   macro avg     0.5022    0.5022    0.5022      4000\n",
      "weighted avg     0.5022    0.5022    0.5022      4000\n",
      "\n",
      "[LABELED] Accuracy: 0.9307 | F1: 0.9593\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8754    0.6847    0.7684       831\n",
      "           1     0.9391    0.9803    0.9593      4121\n",
      "\n",
      "    accuracy                         0.9307      4952\n",
      "   macro avg     0.9072    0.8325    0.8638      4952\n",
      "weighted avg     0.9284    0.9307    0.9272      4952\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.930735056542811, 0.9592781669238989)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_tfidf_lr(X, y, title):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y)\n",
    "    vec = TfidfVectorizer(max_features=50000, ngram_range=(1,2))\n",
    "    Xtr = vec.fit_transform(X_train); Xte = vec.transform(X_test)\n",
    "    clf = LogisticRegression(max_iter=200)\n",
    "    clf.fit(Xtr, y_train)\n",
    "    pred = clf.predict(Xte)\n",
    "    acc = accuracy_score(y_test, pred); f1 = f1_score(y_test, pred)\n",
    "    print(f'[{title}] Accuracy: {acc:.4f} | F1: {f1:.4f}')\n",
    "    print(classification_report(y_test, pred, digits=4))\n",
    "    return acc, f1\n",
    "\n",
    "print('='*70)\n",
    "print('BASELINE TF-IDF + LR')\n",
    "print('='*70)\n",
    "run_tfidf_lr(X_news, y_news, 'NEWS')\n",
    "run_tfidf_lr(X_lab, y_lab, 'LABELED')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT — Fine‑tuning\n",
    "Plus puissant que la baseline, tout en restant accessible sans GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c76119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DISTILBERT FINE-TUNING\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='197' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 197/4000 20:40 < 6:43:07, 0.16 it/s, Epoch 0.10/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.698100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.704700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.695400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=256):\n",
    "        self.enc = tokenizer(list(texts), truncation=True, padding='max_length', max_length=max_len)\n",
    "        self.labels = list(labels)\n",
    "    def __len__(self): return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.enc.items()}\n",
    "        item['labels'] = torch.tensor(int(self.labels[idx]))\n",
    "        return item\n",
    "\n",
    "def train_distilbert(X, y, title, out_dir):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "    train_ds = TextDataset(X_train, y_train, tokenizer)\n",
    "    val_ds = TextDataset(X_val, y_val, tokenizer)\n",
    "\n",
    "    def compute_metrics(p):\n",
    "        preds = np.argmax(p.predictions, axis=1)\n",
    "        acc = accuracy_score(y_val, preds); f1 = f1_score(y_val, preds)\n",
    "        return {'accuracy': acc, 'f1': f1}\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        logging_steps=50,\n",
    "        learning_rate=2e-5\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(model=model, args=args, train_dataset=train_ds, eval_dataset=val_ds, compute_metrics=compute_metrics)\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    print(f\"[{title}] DistilBERT → Accuracy: {metrics['eval_accuracy']:.4f} | F1: {metrics['eval_f1']:.4f}\")\n",
    "    trainer.save_model(out_dir)\n",
    "    return metrics\n",
    "\n",
    "print('='*70)\n",
    "print('DISTILBERT FINE-TUNING')\n",
    "print('='*70)\n",
    "train_distilbert(X_news, y_news, 'NEWS', './models/news_distilbert')\n",
    "train_distilbert(X_lab, y_lab, 'LABELED', './models/labeled_distilbert')\n",
    "\n",
    "print(\"\\n✅ Étape 3 complétée — modèles entraînés et sauvegardés.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project3 venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
